{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the gradient descent algorithm\n",
    "---\n",
    "Let's first use `sklearn` to predict housing prices based on the sample dataset. Then let's implement gradient descent and see if we come up with the same predicted housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients of the hypothesis:  [  6.38433756e-02   1.03436047e+02   0.00000000e+00]\n",
      "predicted: $444.634602, actual: $400.000000\n",
      "predicted: $412.457541, actual: $330.000000\n",
      "predicted: $463.532241, actual: $369.000000\n",
      "predicted: $297.274313, actual: $232.000000\n",
      "predicted: $605.274313, actual: $540.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy.spatial import distance\n",
    "import sklearn\n",
    "from scipy import stats\n",
    "from sklearn.datasets.samples_generator import make_regression \n",
    "\n",
    "living_area = np.array([2104, 1600, 2400, 1416, 3000])\n",
    "num_bedrooms = np.array([3, 3, 3, 2, 4])\n",
    "price = np.array([400, 330, 369, 232, 540]) # we want to predict the price\n",
    "x = np.concatenate((living_area, num_bedrooms), axis = 0)\n",
    "x = np.concatenate((x, np.ones(len(num_bedrooms))), axis = 0) # add in 1s so that we can use theta_0 when computing dot product\n",
    "x = x.reshape(3, 5).T\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.LinearRegression()\n",
    "clf.fit(x, price)\n",
    "\n",
    "print \"coefficients of the hypothesis: \", clf.coef_\n",
    "\n",
    "for i, sample in enumerate(x):\n",
    "    predicted_price = 0\n",
    "    predicted = np.dot(sample, clf.coef_)\n",
    "    print \"predicted: $%f, actual: $%f\" % (predicted, price[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "Gradient descent can be implemented in two main ways: batch gradient descent or stochastic gradient descent. Below is an implementation of batch gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with learning_rate = 1e-09, BGD converged after 412 iterations\n",
      "with learning_rate = 1e-08, BGD converged after 42 iterations\n",
      "\n",
      "coefficients of the hypothesis: [0.17515718101812017, 0.99887165461797678, 0.99963822144941095]\n",
      "predicted: $372.526962, actual: $400.000000\n",
      "predicted: $284.247743, actual: $330.000000\n",
      "predicted: $424.373488, actual: $369.000000\n",
      "predicted: $251.019950, actual: $232.000000\n",
      "predicted: $530.466668, actual: $540.000000\n"
     ]
    }
   ],
   "source": [
    "# X = training dataset, or input data that'll allow us to predict the target dataset (below)\n",
    "# Y = target dataset, or variable we're trying to predict\n",
    "# H = hypothesis, the function that'll predict Y\n",
    "\n",
    "def partial_of_cost_wrt_j(Y, X, theta, H, i, j):\n",
    "    return (Y[i] - H(X[i], theta)) * X[i][j]\n",
    "        \n",
    "def do_gradient_descent(theta, X, Y, H, alpha):\n",
    "    new_theta = [v for v in theta]\n",
    "    for j in range(len(theta)): # for each feature in theta\n",
    "        delta = sum([partial_of_cost_wrt_j(Y, X, theta, H, i, j) for (i, _) in enumerate(Y)])\n",
    "        new_theta[j] += alpha * delta\n",
    "    return new_theta\n",
    "\n",
    "# least squares cost function\n",
    "def cost(X, Y, H, theta):\n",
    "    return 0.5 * sum([ (H(X[i], theta) - Y[i])**2 for (i, _) in enumerate(Y) ])\n",
    "    \n",
    "def fit(X, Y, learning_rate):\n",
    "    def hypothesis(X_i, theta):\n",
    "        return np.dot(X_i, theta)\n",
    "    \n",
    "    theta = np.ones(x.shape[1]) # initialize parameters to all be 1's\n",
    "#     learning_rate = 0.00000001  # this learning rate was empirically determined\n",
    "    c = cost(X, Y, hypothesis, theta)\n",
    "    \n",
    "    num_iterations = 0\n",
    "    while c > 100:\n",
    "        num_iterations += 1\n",
    "#         print \"cost: %f, theta: %s\" % (cost(X, Y, hypothesis, theta), theta)\n",
    "        theta = do_gradient_descent(theta, X, Y, hypothesis, learning_rate)\n",
    "        if abs(cost(X, Y, hypothesis, theta) - c) < 0.001:\n",
    "            break\n",
    "        c = cost(X, Y, hypothesis, theta)\n",
    "    print \"with learning_rate = %s, BGD converged after %d iterations\" % (learning_rate, num_iterations)\n",
    "    return theta\n",
    "\n",
    "coefficients = fit(x, price, 0.000000001)\n",
    "coefficients = fit(x, price, 0.00000001)\n",
    "\n",
    "print\n",
    "print \"coefficients of the hypothesis: %s\" % coefficients\n",
    "for i, sample in enumerate(x):\n",
    "    predicted = np.dot(sample, coefficients)\n",
    "    print \"predicted: $%f, actual: $%f\" % (predicted, price[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `fit` function above uses **batch gradient descent (BGD)** to find the parameters **theta** that result in as low a **cost** as possible. `fit` stops when gradient descent **converges**. Convergence is determined by the programmer. In this case, I printed out the cost after every iteration of BGD, and I determined that it was not changing too much relatively speaking. One value that you need to pay attention to is the **learning rate**. The BGD algorithm **may never converge if the learning rate is too high**. Set the learning rate to be 10x higher, and see what happens to the cost after every iteration of BGD. You'll notice that the cost will increase after every iteration of BGD. This means BGD is taking big strides that are making the algorithm miss a point that'll result in a lower cost. If you set the learning rate to be 10x lower, you'll notice BGD will require more iterations to converge (412 compared to 42).\n",
    "\n",
    "The reason the learning rate needs to be so low is the features of training set vary greatly in terms of value. The first feature is **living area**, which is typically in the thousands. The second features is **number of bedrooms**, which is usually a single digit number (and very rarely in the double digits, unless you're living in a giant mansion). Furthermore, the target values, the prices of the homes, are in the hundreds. With the initial values of theta all set to 1's, the hypothesis will initially evaluate to numbers nearing the thousands because living area is by far the largest feature value. Then, if you subtract the price (which are in the hundreds), you'll still end up with an error (hypothesis - target) in the thousands. As a result, each value in the vector theta will change greatly and wildly if the learning rate is too high. So we must set the learning rate to be low to ensure that theta doesn't change too greatly. If it changes too greatly, intuitively, you can visualize BGD taking huge strides and completely missing values for which theta will bring down the cost.\n",
    "\n",
    "What if we wanted higher learning rates, that is, in the 0.001 to 0.1 range? As explained above, the variance among the ranges of feature values (thousands for living area vs single digit numbers for number of living rooms) is the source of the problem. What if we could assign a score to the living area of a training sample and a score the number of living rooms of that same training sample such that these scores are on the same scale? Fortunately, we can do that with a technique called **feature scaling**. One way to scale a feature would be to divide all feature values by the maximum value for that specific feature among the training set. For example, the maximum living area in our training set is 3000, so we can divide all living areas by 3000. Below, I've calculated the z-score (from statistics) of each feature value. The z-score is used to compare values in statistics, so that is why I'm using it to scale the features. The only value to not scale are the intercept terms (of 1s).\n",
    "\n",
    "Here is a single variate implementation of [gradient descent](http://www.bogotobogo.com/python/python_numpy_batch_gradient_descent_algorithm.php)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with learning_rate = 0.01, BGD converged after 742 iterations\n",
      "with learning_rate = 0.1, BGD converged after 97 iterations\n",
      "with learning_rate = 0.2, BGD converged after 52 iterations\n",
      "\n",
      "coefficients of the hypothesis: [40.689903824446866, 73.052403719768009, 374.19999999999999]\n",
      "predicted: $374.200000, actual: $400.000000\n",
      "predicted: $341.953257, actual: $330.000000\n",
      "predicted: $393.138563, actual: $369.000000\n",
      "predicted: $226.868937, actual: $232.000000\n",
      "predicted: $534.839243, actual: $540.000000\n"
     ]
    }
   ],
   "source": [
    "def scale(X):\n",
    "    all_features = []\n",
    "    for i,features in enumerate(X.T[:-1]):\n",
    "        df = pd.DataFrame(features)\n",
    "        mean = df.mean().get(0)\n",
    "        sigma = df.std().get(0)\n",
    "        scaled_features = [(f - mean) / sigma for f in features]\n",
    "        all_features.append(scaled_features)\n",
    "    all_features.append(X.T[-1])\n",
    "    return np.array(all_features).T\n",
    "\n",
    "coefficients = fit(scale(x), price, 0.01)\n",
    "coefficients = fit(scale(x), price, 0.1)\n",
    "coefficients = fit(scale(x), price, 0.2)\n",
    "\n",
    "print\n",
    "print \"coefficients of the hypothesis: %s\" % coefficients\n",
    "for i, sample in enumerate(scale(x)):\n",
    "    predicted = np.dot(sample, coefficients)\n",
    "    print \"predicted: $%f, actual: $%f\" % (predicted, price[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
